{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-MHvX7XwJobL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring some Functions and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z94eR_uLz1tc"
   },
   "outputs": [],
   "source": [
    "def Graph(Epochs,accuracy,loss):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(range(Epochs), accuracy, color='green')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()    \n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(range(Epochs), loss, color='red')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pcYbGlRU1LVj"
   },
   "outputs": [],
   "source": [
    "def Accuracy_Scores(outputs,binary_labels,correct,total):\n",
    "\n",
    "    scores = precision_recall_fscore_support(binary_labels, outputs, average='weighted')\n",
    "    print(' Precision:', scores[0])\n",
    "    print(' Recall:', scores[1])\n",
    "    print(' F1 Score:', scores[2])  \n",
    "    print(' Accuracy: %d %%' % (100 * correct / total)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yOz15wdH3mYR"
   },
   "outputs": [],
   "source": [
    "A_labels = [\"Actual False\", \"Actual True\"]\n",
    "P_labels = [\"Predicted False\", \"Predicted True\"]   \n",
    "Epochs = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zmoc3cDXY3Zz"
   },
   "outputs": [],
   "source": [
    "data_dir = 'A_05_Part_02_Dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3IRGlYBlJv7D"
   },
   "outputs": [],
   "source": [
    "class ImagePath(datasets.ImageFolder):\n",
    "    \n",
    "    #overriding\n",
    "    def __getitem__(self, index):\n",
    "        original_tuple = super(ImagePath, self).__getitem__(index)\n",
    "        path = self.imgs[index][0]\n",
    "        return (original_tuple + (path,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vV2dcUgVKSIi"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Defining transformations\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "valid_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "# Applying transformations on the data\n",
    "train_data = ImagePath(data_dir + '/Train', transform=train_transforms)\n",
    "valid_data = ImagePath(data_dir + '/Validation', transform=valid_transforms)\n",
    "test_data = ImagePath(data_dir + '/Test', transform=test_transforms)\n",
    "\n",
    "\n",
    "# Data Loaders\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=70, shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(valid_data, batch_size=60, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=60, shuffle=True)\n",
    "\n",
    "\n",
    "print(\"**** Classes ****\")\n",
    "class_names = tuple(train_data.classes)\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CCNchlgCZv2E"
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "def show_databatch(inputs, classes):\n",
    "    out = torchvision.utils.make_grid(inputs)\n",
    "    imshow(out, title=[class_names[x] for x in classes])\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes,paths = next(iter(trainloader))\n",
    "show_databatch(inputs, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gcrsaGZiacg3"
   },
   "source": [
    "# BCE LOGISTLOSS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AlBXalH5as-k"
   },
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XxHVIvsSa2En"
   },
   "source": [
    "### Loading pre-trained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iq6xj0U9a3Df"
   },
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(pretrained=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vFlhE0MBa6qp"
   },
   "source": [
    "### Freezing the layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RQRESiUma89d"
   },
   "outputs": [],
   "source": [
    "for param in vgg16.features.parameters():\n",
    "    param.require_grad = False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M4-iF7RCbCMz"
   },
   "source": [
    "### Modificatiion in Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "pvUtcXxIbEv1",
    "outputId": "c208350a-8b21-4ae5-b9fb-2a55cb87b8a9"
   },
   "outputs": [],
   "source": [
    "# Removing last layer of VGG-16\n",
    "num_features = vgg16.classifier[6].in_features\n",
    "classifier = list(vgg16.classifier.children())[:-1]\n",
    "classifier.extend([nn.Linear(num_features, len(class_names))])\n",
    "vgg16.classifier = nn.Sequential(*classifier)\n",
    "\n",
    "print(vgg16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jn2rlG17bYKY"
   },
   "source": [
    " ### Function & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "w_zR-NjebGka",
    "outputId": "5a0b8707-1621-426c-a22d-fb70c7b8e23e"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "vgg16_optimizer = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3KrcfNbbe8S"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ch10YDJbbQ0"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg16.to(device)\n",
    "\n",
    "vgg16.train()\n",
    "\n",
    "vgg16_accuracy = []\n",
    "\n",
    "vgg16_loss = []\n",
    "\n",
    "# Running Epochs\n",
    "for epoch in range(Epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(trainloader))\n",
    "    for i, data in pbar:\n",
    "        # get the inputs\n",
    "        inputs, labels, paths = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        vgg16_optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = vgg16(inputs)             #----> forward pass\n",
    "        \n",
    "        # One-Hot encoding labels\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "        labels_in_numpy = torch.Tensor.cpu(labels).detach().numpy().reshape(-1, 1)\n",
    "        categories = np.array([[0], [1], [2]])\n",
    "        encoder.fit(categories)\n",
    "        binary_labels = encoder.transform(labels_in_numpy).toarray()\n",
    "           \n",
    "        loss = criterion(outputs, torch.from_numpy(binary_labels).to(device))   #----> compute loss\n",
    "        loss.backward()                     #----> backward pass\n",
    "        vgg16_optimizer.step()              #----> weights update\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        pbar.set_description(\n",
    "            'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(inputs), len(trainloader.dataset),\n",
    "                100. * i / len(trainloader),\n",
    "                loss.data))\n",
    "    \n",
    "    vgg16_loss.append(loss.data)\n",
    "        \n",
    "    torch.save(vgg16.state_dict(), 'vgg16_withoutfocal.pth')\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    covid_Matrix = np.zeros((2,2))\n",
    "    pneu_Matrix = np.zeros((2,2))\n",
    "    normal_Matrix = np.zeros((2,2))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in validloader:\n",
    "            images, labels, paths = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = vgg16(images)\n",
    "            \n",
    "            # Applying sigmoid\n",
    "            outputs = 1 / (1 + np.exp(-torch.Tensor.cpu(outputs).detach().numpy()))\n",
    "            \n",
    "            # Converting to 1 if greater than threshold of 0.5\n",
    "            # if no value is greater than the threshold then defaulting to convert the max value to 1\n",
    "            outputs[:] = np.where(outputs == outputs.max(axis=1).reshape(-1, 1), 1, outputs)\n",
    "            outputs = np.where(outputs >= 0.5, 1, 0)\n",
    "            \n",
    "            # Converting output back to tensor \n",
    "            outputs = torch.from_numpy(outputs).to(device)\n",
    "            \n",
    "            # One-Hot encoding labels\n",
    "            encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "            labels_in_numpy = torch.Tensor.cpu(labels).detach().numpy().reshape(-1, 1)\n",
    "            categories = np.array([[0], [1], [2]])\n",
    "            encoder.fit(categories)\n",
    "            binary_labels = encoder.transform(labels_in_numpy).toarray()\n",
    "            \n",
    "            # Setting pneumonia to 1 if covid-19 is seen\n",
    "            for row in range(binary_labels.shape[0]):\n",
    "                if binary_labels[row][0] == 1:\n",
    "                    binary_labels[row][2] = 1\n",
    "                    \n",
    "            binary_labels = torch.from_numpy(binary_labels).to(device)\n",
    "               \n",
    "            # Converting to numpy arrays for comparison\n",
    "            outputs = torch.Tensor.cpu(outputs).detach().numpy()\n",
    "            binary_labels = torch.Tensor.cpu(binary_labels).detach().numpy()\n",
    "            \n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += np.all(outputs==binary_labels, axis=1).sum()\n",
    "            \n",
    "            # Computing confusion matrices\n",
    "            mat = multilabel_confusion_matrix(binary_labels, outputs)\n",
    "            \n",
    "            ## Making Confusion Matrix\n",
    "\n",
    "            normal_Matrix[0][0] += mat[2][0][0]\n",
    "            normal_Matrix[0][1] += mat[2][0][1]\n",
    "            normal_Matrix[1][0] += mat[2][1][0]\n",
    "            normal_Matrix[1][1] += mat[2][1][1]\n",
    "\n",
    "            covid_Matrix[0][0]  += mat[0][0][0]\n",
    "            covid_Matrix[0][1]  += mat[0][0][1]\n",
    "            covid_Matrix[1][0] += mat[0][1][0]\n",
    "            covid_Matrix[1][1] += mat[0][1][1]\n",
    "            \n",
    "            pneu_Matrix[0][0]  += mat[1][0][0]\n",
    "            pneu_Matrix[0][1] += mat[1][0][1]\n",
    "            pneu_Matrix[1][0] += mat[1][1][0]\n",
    "            pneu_Matrix[1][1] += mat[1][1][1]\n",
    "            \n",
    "\n",
    "    \n",
    "    \n",
    "    vgg16_accuracy.append((100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hz0CMcEsdV5t"
   },
   "source": [
    "### Scores and Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XwXPkcF_dhXY"
   },
   "outputs": [],
   "source": [
    "print('********* VGG16 Confusion Matrices ***********')\n",
    "\n",
    "Accuracy_Scores(outputs,binary_labels,correct,total)\n",
    "   \n",
    "print('\\n\\n Normal ')\n",
    "print(pd.DataFrame(normal_Matrix, A_labels, P_labels))\n",
    "print('\\n\\n Covid-19 ')\n",
    "print(pd.DataFrame(covid_Matrix, A_labels, P_labels))\n",
    "print('\\n\\n Pneumonia ')\n",
    "print(pd.DataFrame(pneu_Matrix, A_labels, P_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1OgylKwmjlJ0"
   },
   "source": [
    "### Accuracy and Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pKEpKg3mjn2H"
   },
   "outputs": [],
   "source": [
    "print('********* VGG16 ***********')\n",
    "\n",
    "Graph(Epochs,vgg16_accuracy,vgg16_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HtnJWWepkAax"
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Amud-UKbkBQh"
   },
   "outputs": [],
   "source": [
    "# Decalring a Dataframe\n",
    "testing_df = pd.DataFrame()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels, paths = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = vgg16(images)\n",
    "        \n",
    "        # Applying sigmoid\n",
    "        outputs = 1 / (1 + np.exp(-torch.Tensor.cpu(outputs).detach().numpy()))\n",
    "            \n",
    "        # Converting to 1 if greater than threshold of 0.5\n",
    "        # if no value is greater than the threshold then defaulting to convert the max value to 1\n",
    "        outputs[:] = np.where(outputs == outputs.max(axis=1).reshape(-1, 1), 1, outputs)\n",
    "        outputs = np.where(outputs >= 0.5, 1, 0)\n",
    "      \n",
    "        Imagepaths = pd.DataFrame(paths)[0].str.split(\"/\", expand = True).iloc[:,-1]\n",
    "        batch_df = pd.concat([Imagepaths, pd.DataFrame(outputs)], axis=1, join='inner')\n",
    "        columns_titles = [9, 0, 2, 1]\n",
    "        batch_df = batch_df.reindex(columns=columns_titles)\n",
    "        batch_df.columns = [''] * len(batch_df.columns)\n",
    "        batch_df.index = [''] * len(batch_df.index)\n",
    "        \n",
    "        testing_df = testing_df.append(batch_df)\n",
    "\n",
    "testing_df.reset_index(drop=True)\n",
    "\n",
    "testing_df.to_csv('MSDS19033_results1.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T0ICmN4SlD1B"
   },
   "source": [
    "## ResNet-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pre-trained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "referenced_widgets": [
      "943a0a47a3334de18be2d930081c0afd",
      "f2868fd44080423cbc8498e577260a37",
      "be23fdb3c2fd4b2ba373157836160e9b",
      "788d08d63a8946e684afe1858afdd519",
      "3c5a3750d33d47929487b7ee8857affa",
      "3ecf4d4548034c1fa9239ef7d128e258",
      "10b51d162bb440fb817d765d974610b7",
      "470f2ff595764db198f4946389e70e19"
     ]
    },
    "colab_type": "code",
    "id": "_aDVxSLKlGVe",
    "outputId": "10353dfe-490c-418b-ec2a-51d4acb9b028"
   },
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C7XXTV6Sldvl"
   },
   "outputs": [],
   "source": [
    "for param in resnet18.parameters():\n",
    "    param.require_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification in Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "colab_type": "code",
    "id": "Vb7E0x80lz-f",
    "outputId": "ee1dd526-a14e-4780-9f0d-79a02b983c48"
   },
   "outputs": [],
   "source": [
    "resnet18.fc = nn.Sequential(OrderedDict([('fc1', nn.Linear(512, len(class_names)))]))\n",
    "print(resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Function & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o13CFH2cl7q8"
   },
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "resnet18_optimizer = optim.SGD(resnet18.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "potB0LHTubWb"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QrKEhKEXt_bf"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet18.to(device)\n",
    "\n",
    "resnet18.train()\n",
    "\n",
    "resnet18_accuracy = []\n",
    "\n",
    "resnet18_loss = []\n",
    "\n",
    "# Running Epochs\n",
    "for epoch in range(Epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(trainloader))\n",
    "    for i, data in pbar:\n",
    "        # get the inputs\n",
    "        inputs, labels, paths = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        resnet18_optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = resnet18(inputs)             #----> forward pass\n",
    "        \n",
    "        # One-Hot encoding labels\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "        labels_in_numpy = torch.Tensor.cpu(labels).detach().numpy().reshape(-1, 1)\n",
    "        categories = np.array([[0], [1], [2]])\n",
    "        encoder.fit(categories)\n",
    "        binary_labels = encoder.transform(labels_in_numpy).toarray()\n",
    "           \n",
    "        loss = criterion(outputs, torch.from_numpy(binary_labels).to(device))   #----> compute loss\n",
    "        loss.backward()                     #----> backward pass\n",
    "        resnet18_optimizer.step()              #----> weights update\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        pbar.set_description(\n",
    "            'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(inputs), len(trainloader.dataset),\n",
    "                100. * i / len(trainloader),\n",
    "                loss.data))\n",
    "    \n",
    "    resnet18_loss.append(loss.data)\n",
    "        \n",
    "    torch.save(resnet18.state_dict(), 'resnet18_withoutfocal.pth')\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    covid_Matrix = np.zeros((2,2))\n",
    "    pneu_Matrix = np.zeros((2,2))\n",
    "    normal_Matrix = np.zeros((2,2))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in validloader:\n",
    "            images, labels, paths = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = vgg16(images)\n",
    "            \n",
    "            # Applying sigmoid\n",
    "            outputs = 1 / (1 + np.exp(-torch.Tensor.cpu(outputs).detach().numpy()))\n",
    "            \n",
    "            # Converting to 1 if greater than threshold of 0.5\n",
    "            # if no value is greater than the threshold then defaulting to convert the max value to 1\n",
    "            outputs[:] = np.where(outputs == outputs.max(axis=1).reshape(-1, 1), 1, outputs)\n",
    "            outputs = np.where(outputs >= 0.5, 1, 0)\n",
    "            \n",
    "            # Converting output back to tensor \n",
    "            outputs = torch.from_numpy(outputs).to(device)\n",
    "            \n",
    "            # One-Hot encoding labels\n",
    "            encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "            labels_in_numpy = torch.Tensor.cpu(labels).detach().numpy().reshape(-1, 1)\n",
    "            categories = np.array([[0], [1], [2]])\n",
    "            encoder.fit(categories)\n",
    "            binary_labels = encoder.transform(labels_in_numpy).toarray()\n",
    "            \n",
    "            # Setting pneumonia to 1 if covid-19 is seen\n",
    "            for row in range(binary_labels.shape[0]):\n",
    "                if binary_labels[row][0] == 1:\n",
    "                    binary_labels[row][2] = 1\n",
    "                    \n",
    "            binary_labels = torch.from_numpy(binary_labels).to(device)\n",
    "               \n",
    "            # Converting to numpy arrays for comparison\n",
    "            outputs = torch.Tensor.cpu(outputs).detach().numpy()\n",
    "            binary_labels = torch.Tensor.cpu(binary_labels).detach().numpy()\n",
    "            \n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += np.all(outputs==binary_labels, axis=1).sum()\n",
    "            \n",
    "            # Computing confusion matrices\n",
    "            mat = multilabel_confusion_matrix(binary_labels, outputs)\n",
    "            \n",
    "            ## Making Confusion Matrix\n",
    "\n",
    "            normal_Matrix[0][0] += mat[2][0][0]\n",
    "            normal_Matrix[0][1] += mat[2][0][1]\n",
    "            normal_Matrix[1][0] += mat[2][1][0]\n",
    "            normal_Matrix[1][1] += mat[2][1][1]\n",
    "\n",
    "\n",
    "            covid_Matrix[0][0]  += mat[0][0][0]\n",
    "            covid_Matrix[0][1]  += mat[0][0][1]\n",
    "            covid_Matrix[1][0] += mat[0][1][0]\n",
    "            covid_Matrix[1][1] += mat[0][1][1]\n",
    "            \n",
    "            pneu_Matrix[0][0]  += mat[1][0][0]\n",
    "            pneu_Matrix[0][1] += mat[1][0][1]\n",
    "            pneu_Matrix[1][0] += mat[1][1][0]\n",
    "            pneu_Matrix[1][1] += mat[1][1][1]\n",
    "            \n",
    "\n",
    "    \n",
    "    \n",
    "    resnet18_accuracy.append((100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BAkZAY8dukKF"
   },
   "source": [
    "### Scores and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XnZgfQY7uoA7"
   },
   "outputs": [],
   "source": [
    "print('********* ResNet18 Confusion Matrices *********')\n",
    "Accuracy_Scores(outputs,binary_labels,correct,total)\n",
    "\n",
    "\n",
    "print('\\n\\n Normal ')\n",
    "print(pd.DataFrame(normal_Matrix, A_labels, P_labels))\n",
    "print('\\n\\n Covid-19 ')\n",
    "print(pd.DataFrame(covid_Matrix, A_labels, P_labels))\n",
    "print('\\n\\n Pneumonia ')\n",
    "print(pd.DataFrame(pneu_Matrix, A_labels, P_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxMwhikFu_3q"
   },
   "source": [
    "### Accuracy and Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ExqJ7dUqvAzv"
   },
   "outputs": [],
   "source": [
    "print('********* ResNet18 *********')\n",
    "\n",
    "Graph(Epochs,resnet18_accuracy,resnet18_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "roIU35v7vRjL"
   },
   "outputs": [],
   "source": [
    "# Decalring a Dataframe\n",
    "testing_df = pd.DataFrame()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels, paths = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = resnet18(images)\n",
    "        \n",
    "        # Applying sigmoid\n",
    "        outputs = 1 / (1 + np.exp(-torch.Tensor.cpu(outputs).detach().numpy()))\n",
    "            \n",
    "        # Converting to 1 if greater than threshold of 0.5\n",
    "        # if no value is greater than the threshold then defaulting to convert the max value to 1\n",
    "        outputs[:] = np.where(outputs == outputs.max(axis=1).reshape(-1, 1), 1, outputs)\n",
    "        outputs = np.where(outputs >= 0.5, 1, 0)\n",
    "      \n",
    "        Imagepaths = pd.DataFrame(paths)[0].str.split(\"/\", expand = True).iloc[:,-1]\n",
    "        batch_df = pd.concat([Imagepaths, pd.DataFrame(outputs)], axis=1, join='inner')\n",
    "        columns_titles = [9, 0, 2, 1]\n",
    "        batch_df = batch_df.reindex(columns=columns_titles)\n",
    "        batch_df.columns = [''] * len(batch_df.columns)\n",
    "        batch_df.index = [''] * len(batch_df.index)\n",
    "        \n",
    "        testing_df = testing_df.append(batch_df)\n",
    "\n",
    "testing_df.reset_index(drop=True)\n",
    "\n",
    "testing_df.to_csv('MSDS19033_results2.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ubhtyAZWxf2Y"
   },
   "source": [
    "# Focal Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x3bXr4X4xtDN"
   },
   "source": [
    "## VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hbi99NjBxqx8"
   },
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(pretrained=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0MWOwsc3xwbq"
   },
   "source": [
    "### Freezing Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fMStCYSlxyTh"
   },
   "outputs": [],
   "source": [
    "for param in vgg16.features.parameters():\n",
    "    param.require_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vospi7UOx2R-"
   },
   "source": [
    "### Modification In layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Ydj38Aox4nN"
   },
   "outputs": [],
   "source": [
    "num_features = vgg16.classifier[6].in_features\n",
    "classifier = list(vgg16.classifier.children())[:-1]\n",
    "classifier.extend([nn.Linear(num_features, len(class_names))])\n",
    "vgg16.classifier = nn.Sequential(*classifier)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "        \n",
    "    def forward(self, outputs, labels):\n",
    "        outputs_sigmoid = torch.sigmoid(outputs)\n",
    "        BCE_loss = F.binary_cross_entropy(outputs_sigmoid, labels, reduce=False)   \n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * torch.Tensor.cpu((1-pt)**self.gamma * BCE_loss).detach().numpy()\n",
    "        return np.mean(F_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-QjcM7dBx-2d"
   },
   "source": [
    " ### Function & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Ms_xdALyBPa"
   },
   "outputs": [],
   "source": [
    "\n",
    "criterion = FocalLoss([0.4, 0.3, 0.2], 2)\n",
    "vgg16_optimizer = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ymdnL6Fyyod5"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kcJHyXuSypN4"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg16.to(device)\n",
    "\n",
    "vgg16.train()\n",
    "\n",
    "\n",
    "vgg16_loss = []\n",
    "vgg16_accuracy = []\n",
    "\n",
    "# Running Epochs\n",
    "for epoch in range(Epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(trainloader))\n",
    "    for i, data in pbar:\n",
    "        # get the inputs\n",
    "        inputs, labels, paths = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        vgg16_optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = vgg16(inputs)               #----> forward pass\n",
    "        \n",
    "        # One-Hot encoding labels\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "        labels_in_numpy = torch.Tensor.cpu(labels).detach().numpy().reshape(-1, 1)\n",
    "        categories = np.array([[0], [1], [2]])\n",
    "        encoder.fit(categories)\n",
    "        binary_labels = encoder.transform(labels_in_numpy).toarray()\n",
    "        \n",
    "        outputs_numpy = torch.Tensor.cpu(outputs).detach().numpy()\n",
    "        outputs = torch.from_numpy(binary_labels).to(device)\n",
    "        labels_numpy = torch.Tensor.cpu(torch.from_numpy(binary_labels)).detach().numpy()\n",
    "         \n",
    "        loss = criterion(outputs, torch.from_numpy(binary_labels).to(device))   #----> compute loss\n",
    "        loss = torch.from_numpy(np.asarray(loss)).to(device)\n",
    "        loss = Variable(loss, requires_grad = True)\n",
    "        loss.backward()\n",
    "        vgg16_optimizer.step()                    #----> weights update\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        pbar.set_description(\n",
    "            'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(inputs), len(trainloader.dataset),\n",
    "                100. * i / len(trainloader),\n",
    "                loss))\n",
    "        \n",
    "    vgg16_loss.append(loss.data)\n",
    "        \n",
    "    torch.save(vgg16.state_dict(), 'vgg16_focal_loss.pth')\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    covid_Matrix = np.zeros((2,2))\n",
    "    pneu_Matrix = np.zeros((2,2))\n",
    "    normal_Matrix = np.zeros((2,2))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in validloader:\n",
    "            images, labels, paths = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = vgg16(images)\n",
    "            \n",
    "            # Applying sigmoid\n",
    "            outputs = 1 / (1 + np.exp(-torch.Tensor.cpu(outputs).detach().numpy()))\n",
    "            \n",
    "            # Converting to 1 if greater than threshold of 0.5\n",
    "            # if no value is greater than the threshold then defaulting to convert the max value to 1\n",
    "            outputs[:] = np.where(outputs == outputs.max(axis=1).reshape(-1, 1), 1, outputs)\n",
    "            outputs = np.where(outputs >= 0.5, 1, 0)\n",
    "            \n",
    "            # Converting output back to tensor \n",
    "            outputs = torch.from_numpy(outputs).to(device)\n",
    "            \n",
    "            # One-Hot encoding labels\n",
    "            encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "            labels_in_numpy = torch.Tensor.cpu(labels).detach().numpy().reshape(-1, 1)\n",
    "            categories = np.array([[0], [1], [2]])\n",
    "            encoder.fit(categories)\n",
    "            binary_labels = encoder.transform(labels_in_numpy).toarray()\n",
    "            \n",
    "            # Setting pneumonia to 1 if covid-19 is seen\n",
    "            for row in range(binary_labels.shape[0]):\n",
    "                if binary_labels[row][0] == 1:\n",
    "                    binary_labels[row][2] = 1\n",
    "                    \n",
    "            binary_labels = torch.from_numpy(binary_labels).to(device)\n",
    "               \n",
    "            # Converting to numpy arrays for comparison\n",
    "            outputs = torch.Tensor.cpu(outputs).detach().numpy()\n",
    "            binary_labels = torch.Tensor.cpu(binary_labels).detach().numpy()\n",
    "            \n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += np.all(outputs==binary_labels, axis=1).sum()\n",
    "            \n",
    "            # Computing confusion matrices\n",
    "            mat = multilabel_confusion_matrix(binary_labels, outputs)\n",
    "            \n",
    "            ## Making Confusion Matrix\n",
    "\n",
    "            normal_Matrix[0][0] += mat[2][0][0]\n",
    "            normal_Matrix[0][1] += mat[2][0][1]\n",
    "            normal_Matrix[1][0] += mat[2][1][0]\n",
    "            normal_Matrix[1][1] += mat[2][1][1]\n",
    "            covid_Matrix[0][0]  += mat[0][0][0]\n",
    "            covid_Matrix[0][1]  += mat[0][0][1]\n",
    "            covid_Matrix[1][0] += mat[0][1][0]\n",
    "            covid_Matrix[1][1] += mat[0][1][1]\n",
    "            pneu_Matrix[0][0]  += mat[1][0][0]\n",
    "            pneu_Matrix[0][1] += mat[1][0][1]\n",
    "            pneu_Matrix[1][0] += mat[1][1][0]\n",
    "            pneu_Matrix[1][1] += mat[1][1][1]\n",
    "            \n",
    "\n",
    "    \n",
    "    \n",
    "    vgg16_accuracy.append((100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TXiGI9sdzlhd"
   },
   "outputs": [],
   "source": [
    "    \n",
    "print('********* VGG-16 Focal Loss Confusion Matrices*********')\n",
    "Accuracy_Scores(outputs,binary_labels,correct,total)\n",
    " \n",
    "print('\\n\\n Normal ')\n",
    "print(pd.DataFrame(normal_Matrix, A_labels, P_labels))\n",
    "print('\\n\\n Covid-19 ')\n",
    "print(pd.DataFrame(covid_Matrix, A_labels, P_labels))\n",
    "print('\\n\\n Pneumonia ')\n",
    "print(pd.DataFrame(pneu_Matrix, A_labels, P_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T6Q5s-Wmztxv"
   },
   "outputs": [],
   "source": [
    "print('********* VGG-16 Focal Loss*********')\n",
    "\n",
    "Graph(Epochs,vgg16_accuracy,vgg16_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5881-4Rz2C_a"
   },
   "outputs": [],
   "source": [
    "# Decalring a Dataframe\n",
    "testing_df = pd.DataFrame()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels, paths = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = vgg16(images)\n",
    "        \n",
    "        # Applying sigmoid\n",
    "        outputs = 1 / (1 + np.exp(-torch.Tensor.cpu(outputs).detach().numpy()))\n",
    "            \n",
    "        # Converting to 1 if greater than threshold of 0.5\n",
    "        # if no value is greater than the threshold then defaulting to convert the max value to 1\n",
    "        outputs[:] = np.where(outputs == outputs.max(axis=1).reshape(-1, 1), 1, outputs)\n",
    "        outputs = np.where(outputs >= 0.5, 1, 0)\n",
    "      \n",
    "        Imagepaths = pd.DataFrame(paths)[0].str.split(\"/\", expand = True).iloc[:,-1]\n",
    "        batch_df = pd.concat([Imagepaths, pd.DataFrame(outputs)], axis=1, join='inner')\n",
    "        columns_titles = [9, 0, 2, 1]\n",
    "        batch_df = batch_df.reindex(columns=columns_titles)\n",
    "        batch_df.columns = [''] * len(batch_df.columns)\n",
    "        batch_df.index = [''] * len(batch_df.index)\n",
    "        \n",
    "        testing_df = testing_df.append(batch_df)\n",
    "\n",
    "testing_df.reset_index(drop=True)\n",
    "\n",
    "testing_df.to_csv('MSDS19033_results3.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kfah-IGE2EKZ"
   },
   "source": [
    "# ResNet-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qw_eKvlF2FYb"
   },
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z063YQI72OwH"
   },
   "outputs": [],
   "source": [
    "for param in resnet18.parameters():\n",
    "    param.require_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "id": "RoPqKVUB2UnX",
    "outputId": "d7bd19b2-6170-4363-a69c-20bfc6b2e644"
   },
   "outputs": [],
   "source": [
    "resnet18.fc = nn.Sequential(OrderedDict([('fc1', nn.Linear(512, len(class_names)))]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nV8irQNl2dPj"
   },
   "outputs": [],
   "source": [
    "\n",
    "criterion = FocalLoss([0.4, 0.3, 0.2], 2)\n",
    "resnet18_optimizer = optim.SGD(resnet18.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P6RvtiRm2gzA"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GNyWn-1G2hgS"
   },
   "outputs": [],
   "source": [
    "# Setting to either GPU or CPU based on availability\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet18.to(device)\n",
    "\n",
    "resnet18.train()\n",
    "\n",
    "\n",
    "resnet18_cross_entropy = []\n",
    "resnet18_valid_accuracy = []\n",
    "\n",
    "# Running Epochs\n",
    "for epoch in range(Epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(trainloader))\n",
    "    for i, data in pbar:\n",
    "        # get the inputs\n",
    "        inputs, labels, paths = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        resnet18_optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = resnet18(inputs)               #----> forward pass\n",
    "                \n",
    "        # One-Hot encoding labels\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "        labels_in_numpy = torch.Tensor.cpu(labels).detach().numpy().reshape(-1, 1)\n",
    "        categories = np.array([[0], [1], [2]])\n",
    "        encoder.fit(categories)\n",
    "        binary_labels = encoder.transform(labels_in_numpy).toarray()\n",
    "        \n",
    "        outputs_numpy = torch.Tensor.cpu(outputs).detach().numpy()\n",
    "        outputs = torch.from_numpy(binary_labels).to(device)\n",
    "        labels_numpy = torch.Tensor.cpu(torch.from_numpy(binary_labels)).detach().numpy()\n",
    "         \n",
    "        loss = criterion(outputs, torch.from_numpy(binary_labels).to(device))   #----> compute loss\n",
    "        loss = torch.from_numpy(np.asarray(loss)).to(device)\n",
    "        loss = Variable(loss, requires_grad = True)\n",
    "        \n",
    "        loss.backward()                     #----> backward pass\n",
    "        resnet18_optimizer.step()                    #----> weights update\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        pbar.set_description(\n",
    "            'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(inputs), len(trainloader.dataset),\n",
    "                100. * i / len(trainloader),\n",
    "                loss.data))\n",
    "        \n",
    "    resnet18_cross_entropy.append(loss.data)\n",
    "        \n",
    "    torch.save(resnet18.state_dict(), 'res18_focal_loss.pth')\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    covid_Matrix = np.zeros((2,2))\n",
    "    pneu_Matrix = np.zeros((2,2))\n",
    "    normal_Matrix = np.zeros((2,2))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in validloader:\n",
    "            images, labels, paths = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = resnet18(images)\n",
    "            \n",
    "            # Applying sigmoid\n",
    "            outputs = 1 / (1 + np.exp(-torch.Tensor.cpu(outputs).detach().numpy()))\n",
    "            \n",
    "            # Converting to 1 if greater than threshold of 0.5\n",
    "            # if no value is greater than the threshold then defaulting to convert the max value to 1\n",
    "            outputs[:] = np.where(outputs == outputs.max(axis=1).reshape(-1, 1), 1, outputs)\n",
    "            outputs = np.where(outputs >= 0.5, 1, 0)\n",
    "            \n",
    "            # Converting output back to tensor \n",
    "            outputs = torch.from_numpy(outputs).to(device)\n",
    "            \n",
    "            # One-Hot encoding labels\n",
    "            encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "            labels_in_numpy = torch.Tensor.cpu(labels).detach().numpy().reshape(-1, 1)\n",
    "            categories = np.array([[0], [1], [2]])\n",
    "            encoder.fit(categories)\n",
    "            binary_labels = encoder.transform(labels_in_numpy).toarray()\n",
    "            \n",
    "            # Setting pneumonia to 1 if covid-19 is seen\n",
    "            for row in range(binary_labels.shape[0]):\n",
    "                if binary_labels[row][0] == 1:\n",
    "                    binary_labels[row][2] = 1\n",
    "                    \n",
    "            binary_labels = torch.from_numpy(binary_labels).to(device)\n",
    "               \n",
    "            # Converting to numpy arrays for comparison\n",
    "            outputs = torch.Tensor.cpu(outputs).detach().numpy()\n",
    "            binary_labels = torch.Tensor.cpu(binary_labels).detach().numpy()\n",
    "            total += labels.size(0)\n",
    "            correct += np.all(outputs==binary_labels, axis=1).sum()\n",
    "            \n",
    "            # Computing confusion matrices\n",
    "            mat = multilabel_confusion_matrix(binary_labels, outputs)\n",
    "            \n",
    "            ## Making Confusion Matrix\n",
    "\n",
    "            normal_Matrix[0][0] += mat[2][0][0]\n",
    "            normal_Matrix[0][1] += mat[2][0][1]\n",
    "            normal_Matrix[1][0] += mat[2][1][0]\n",
    "            normal_Matrix[1][1] += mat[2][1][1]\n",
    "            covid_Matrix[0][0]  += mat[0][0][0]\n",
    "            covid_Matrix[0][1]  += mat[0][0][1]\n",
    "            covid_Matrix[1][0] += mat[0][1][0]\n",
    "            covid_Matrix[1][1] += mat[0][1][1]\n",
    "            pneu_Matrix[0][0]  += mat[1][0][0]\n",
    "            pneu_Matrix[0][1] += mat[1][0][1]\n",
    "            pneu_Matrix[1][0] += mat[1][1][0]\n",
    "            pneu_Matrix[1][1] += mat[1][1][1]\n",
    "            \n",
    "\n",
    "    \n",
    "    \n",
    "    resnet18_accuracy.append((100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jMDK_OK126iM"
   },
   "outputs": [],
   "source": [
    "print('********* ResNet18 Confusion Matrices*********')\n",
    "Accuracy_Scores(outputs,binary_labels,correct,total)\n",
    "\n",
    "print('\\n\\n Normal ')\n",
    "print(pd.DataFrame(normal_Matrix, A_labels, P_labels))\n",
    "print('\\n\\n Covid-19 ')\n",
    "print(pd.DataFrame(covid_Matrix, A_labels, P_labels))\n",
    "print('\\n\\n Pneumonia ')\n",
    "print(pd.DataFrame(pneu_Matrix, A_labels, P_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zGuy23dq3Asg"
   },
   "outputs": [],
   "source": [
    "print('********* ResNet18 *********')\n",
    "Graph(Epochs,resnet18_accuracy,resnet18_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BxlmxVIo3RGN"
   },
   "outputs": [],
   "source": [
    "# Declaring a Dataframe\n",
    "testing_df = pd.DataFrame()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels, paths = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = resnet18(images)\n",
    "        \n",
    "        # Applying sigmoid\n",
    "        outputs = 1 / (1 + np.exp(-torch.Tensor.cpu(outputs).detach().numpy()))\n",
    "            \n",
    "        # Converting to 1 if greater than threshold of 0.5\n",
    "        # if no value is greater than the threshold then defaulting to convert the max value to 1\n",
    "        outputs[:] = np.where(outputs == outputs.max(axis=1).reshape(-1, 1), 1, outputs)\n",
    "        outputs = np.where(outputs >= 0.5, 1, 0)\n",
    "      \n",
    "        Imagepaths = pd.DataFrame(paths)[0].str.split(\"/\", expand = True).iloc[:,-1]\n",
    "        batch_df = pd.concat([Imagepaths, pd.DataFrame(outputs)], axis=1, join='inner')\n",
    "        columns_titles = [9, 0, 2, 1]\n",
    "        batch_df = batch_df.reindex(columns=columns_titles)\n",
    "        batch_df.columns = [''] * len(batch_df.columns)\n",
    "        batch_df.index = [''] * len(batch_df.index)\n",
    "        \n",
    "        testing_df = testing_df.append(batch_df)\n",
    "\n",
    "testing_df.reset_index(drop=True)\n",
    "\n",
    "testing_df.to_csv('MSDS19033_results4.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HaseebAssignment5PartB.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "10b51d162bb440fb817d765d974610b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3c5a3750d33d47929487b7ee8857affa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3ecf4d4548034c1fa9239ef7d128e258": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "470f2ff595764db198f4946389e70e19": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "788d08d63a8946e684afe1858afdd519": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_470f2ff595764db198f4946389e70e19",
      "placeholder": "​",
      "style": "IPY_MODEL_10b51d162bb440fb817d765d974610b7",
      "value": " 44.7M/44.7M [1:09:08&lt;00:00, 11.3kB/s]"
     }
    },
    "943a0a47a3334de18be2d930081c0afd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_be23fdb3c2fd4b2ba373157836160e9b",
       "IPY_MODEL_788d08d63a8946e684afe1858afdd519"
      ],
      "layout": "IPY_MODEL_f2868fd44080423cbc8498e577260a37"
     }
    },
    "be23fdb3c2fd4b2ba373157836160e9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ecf4d4548034c1fa9239ef7d128e258",
      "max": 46827520,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3c5a3750d33d47929487b7ee8857affa",
      "value": 46827520
     }
    },
    "f2868fd44080423cbc8498e577260a37": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
